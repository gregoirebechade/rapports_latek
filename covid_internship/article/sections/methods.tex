\section{Methods}

\subsection*{The data}

As the goal of this study is to compare some forecasts on many different pandemics, many synthetic pandemics need to be generated, with a particular attention on the diversity of these pandemics. 

\subsubsection*{Covasim}

To generate the pandemics,  \cite{kerr2021covasim}, a python librairy that can simulate the evolution of a pandemic was used. 
Covasim is an agent-based model that can model many different pandemics and has a high diversity of outputs. 
This model takes as an input many parameters such as the population type, the population size, the age repartition \dots and outputs a complete description of the pandemic, with real-time values of each relevant information, sucha as the number of severe, of asymptomatic... but also physical values such as the value of the reproduction number. 
Covasim enables to generate a huge diversity of pandemics, thanks to the plurality of parameters that can be given as the input of the model, but also with interventions that can be planned by the users. 
These interventions can simulate the impact of a vaccination campaign, with changes in the probability transmission, that can be different for all ages groups. 

\subsubsection{First pandemics}

For the implementation and the first test of the models, two pandemics were generated. 
The first one focusing on the new deaths count and the second one focusing on the number of hospitalized count. 
Those pandemics will be referred to as pandemic 1 and pandemic 2. 

\input{tables/table_covasim_parameters.tex}

The parameters used to generate these pandemics are described in the table \ref{tab:parameters_covasim}.
The parameters that are not specified are the default parameters of the Covasim librairy. 


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mobility_reports.png}
    \caption{Mobility reports from Västtrafik.}
    \label{fig:mobility_reports}
\end{figure}


The different interventions were based on mobility reports from Västtrafik  \ref{fig:mobility_reports}, the public transport company of the city of Gothenburg, which were reported during the Covid 19 pandemic and has been retrieved in \cite{gerlee2021predicting}.
These interventions correspond to 53 relative weekly variations of the mobility, with a reference value of 1 for the first week of the report, which correspond to the 9-th week of 2020. 


\subsubsection{Generating diverse pandemics}

In order to evaluate the performances of our models on a wide range of pandemics, a training set of pandemics was generated. 
A huge diversity of pandemics is needed to determine which model is the more consistent.
It is so relevant to identify the key parameters that generate this diversity.
As Covasim has a very huge set of inputs parameters, a first subset of key parameters was identified: the spread parameters and the severity parameters. 
The severity parameters are the 4 parameters that correspond to the probability for an agent to get from a compartment to another. 
The spread parameters are 9 parameters that represent the distribution of probability of the time spend by an agent in a compartment (such as infected, crictical...) once he entered it. 
This distribution is a log-normal distribution, but the spread-parameters correspond to the mean of this log-normal distribution
All the parameters have a default value of 1, which correspond to keeping the reference value. 
We decided to select 4 parameters and to make them vary in $[0.5, 1, 2]$, leading to a set of 81 pandemics. 
To select the 4 parameters that generated the most diversity, different diversity metrics were computed. 

Let $Y_1$ ,  $Y_2 \in \mathbb{R}^n$  be two time series of $n$ days representing the number of hospitalized in two pandemics.  

Let :  \\

$
\begin{aligned}
    &\mathcal{L}_1(Y_1, Y_2) = \| Y_1 - Y_2 \|_{L_1}  \\
    &\mathcal{L}_2(Y_1, Y_2) = \| (\frac{max(Y_1)}{max(Y_2)} ;\frac{max(Y_1')}{max(Y_2')} ; \frac{max(Y_1'')}{max(Y_2'')} , \| \tilde{Y_1} - \tilde{Y_2} \|_{L_1}, \| \tilde{Y_1'} - \tilde{Y_2'} \|_{L_1} , \| \tilde{Y_1''} - \tilde{Y_2''} \|_{L_1} ) \|_{L_2} \\
    &\quad \quad\quad\quad\quad \quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\text{ with }Y' \text{ and } Y'' \text{the first and second derivatives of} Y\\
    &\mathcal{L}_3 = \mathcal{W}(\tilde{Y_1} - \tilde{Y_2}) \text{, with }\mathcal{W} \text{ the Wasserstein distance.} \\
    &\mathcal{L}_4(Y_1, Y_2) = \| (\frac{max(Y_1)}{max(Y_2)} ;\frac{max(Y_1')}{max(Y_2')} ; \frac{max(Y_1'')}{max(Y_2'')} , \mathcal{W} (\tilde{Y_1} - \tilde{Y_2} ),\mathcal{W} ( \tilde{Y_1'} - \tilde{Y_2'} ) , \mathcal{W} (\tilde{Y_1''} - \tilde{Y_2''} ) ) \|_{L_2} \\
    &\quad \quad\quad\quad\quad \quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\quad \quad\quad\quad\text{ with }Y' \text{ and } Y'' \text{the first and second derivatives of} Y\\[1cm]
\end{aligned}
$

It can be noted that $\mathcal{L}_2$ looks like the Sobolev norm  $\| \tilde{Y_1}  - \tilde{Y_2} \|_{W^{2, 1}} $ with squared terms and with additionary terms taking into account the amplitude. 

To determine which measure to use, we generated 14 pandemics. 
Each pandemic but the last one has default parameters except one of them which was doubled. 
The last pandemic has only default parameters. 

For each norm $\mathcal{L}_k$, we determined $S$,  the subset of 4 pandemics that maximized the following quantity:\\

\[ \sum_{i, j \in S, i \neq j} \mathcal{L}_k(Y_i, Y_j) \]

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/most_different_pandemics.png}
    \caption{4 most diverse pandemics according to each norm.}
    \label{fig:diversity_pandemics}
\end{figure}


The 4 most divers pandemics accorded to each norm are shown in the figure \ref{fig:diversity_pandemics}.
We decided according to this figure, that $\mathcal{L}_2$ norm was the most relevant to determine the diversity of the pandemics.
But, keeping the parameters \texttt{[0, 5, 10, 12]} would not be accurate, as the parameters were changed independantly, and the diversity did not take into account the correlation between some of them. 

Finding the parameters that maximise the $\mathcal{L}_2$ diversity is equivalent to solve the following problem : \\

$ S_{opt} = \underset{S' \subset S , \vert S' \vert =4 }{argmax} \mathcal{L}(S')$, with $ \mathcal{L}(S')= \sum_{s, t \in \mathcal{P}_g(S') , s \neq t }{\mathcal{L}_2(s,t)}$, and  $\mathcal{P}_{g}(S')$ the set of the 81 pandemics generated with the 4 parameters of $S'$

However, generating a pandemic with \texttt{Covasim} is time consuming, and it is not possible to compute the diversity of each set of 4 parameters $S'$ included in $S$.\\[1cm]


A MCMC algorithm \cite*{diaconis2009markov} was then implemented, to perform a clever grid search on the different subsets $S' \subset S $ of parameters.
The MCMC algorithm is a method which is used to sample from a distribution that can't be directly sampled. 
The main idea is to construct a Markov Chain whose stationary distribution is the objective distribution.

Let $\mathcal{S} = \{ S'\subset S ;  \vert S' \vert = 4 \}$ be the support of the target distribution, which is, in our case, the set of all the 715 combinations of the 4 parameters among the 13 different possible, and let $\pi$ be the target distribution on $\mathcal{S}$.
$\forall s \in \mathcal{S}, \pi(s) = \frac{\mathcal{L}(s)}{\sum_{s \in \mathcal{S}}  \mathcal{L}(s)} $. $\pi$ is not directly computable as it is too time consuming to compute the denominator.


For each $s = [a, b, c, d] \in \mathcal{S}$, let $ne(s)$ be the set of the neighbourghs of $s$, i.e the set of all the elements of $\mathcal{S}$ who have only one parameter different from $s$. 
For instance, $[0, 3, 9, 12] \in ne([0, 3, 10, 12])$, but $[0, 3, 9, 12] \notin ne([0, 3, 8, 10])$.\\

Let $U_n$ be a sequence of independent uniform random variables on $[0, 1]$ and $\forall s \in \mathcal{S}$, let $U_{n}^{ne(s)}$ be a sequence of independant uniform random variables on $ne(s)$ .
Let $s_0 \in \mathcal{S}$ and let  $S_n$ be the random sequence defined as follow : \\


\[
 \left\{
    \begin{aligned}
        & S_0 = s_0 \\
        & \forall n \in \mathbb{N}, \alpha_n = \frac{\mathcal{L}(U_{n}^{ne(S_n)})}{\mathcal{L}(\mathcal{S}_n)} \\
        & \forall n \in \mathbb{N}, S_{n+1} = U_{n}^{ne(S_n)} \mathbb{1}_{\{U_n < \alpha_n\}} + X_n \mathbb{1}_{\{U_n > \alpha_n\}}
\end{aligned}
\right.
\]
\\[0.5cm]

As $S_{n+1}$ is a function of $S_n$ and of other independant random variables, the sequence $S_n$ is a homogenous Markov Chain.
This formula means that at each iteration, a neighbourgh of $S_n$ is uniformly selected among all the neighbourghs of $S_n$ (it is $U_n^{ne(S_n)}$)
The Markov Chain moves to this neighbourgh if the value of  $\mathcal{L}(U_n^{ne(S_n)})$ is higher than the value of the function $\mathcal{L}(S_n)$ at the current state.
If the new value of $\mathcal{L}$ is smaller, the markov chain moves with a probability that is equal to the ratio of the two values.
This way of moving on the different subsets prevents to be stucked in a local maxima but avoids exploring dummies areas, in which the diversity is very small. \\

The transition matrix of this Markov Chain is the following: 
\[
K(s, s'): \left\{
\begin{aligned}
& 0 \text{ if } s' \notin ne(s) \text{ and } s' \neq s   \\
& \frac{1}{Card(ne(s))} = \frac{1}{36} \text{ if } s' \in ne(s) \text{ and } \frac{\mathcal{L}(s')}{\mathcal{L}(s)} > 1 \text{ and } s' \neq s \\
& \frac{1}{36} \times \frac{\mathcal{L}(s')}{\mathcal{L}(s)} \text{ if } s' \in ne(s) \text{ and } \frac{\mathcal{L}(s')}{\mathcal{L}(s)} \leq 1  \text{ and } s' \neq s  \\
& 1 - \sum_{s' \in \mathcal{S}, s' \neq s}K(s, s') \text{ if } s' = s 
\end{aligned}
\right.
\]
\\[0.3cm]

Let $(s, s') \in \mathcal{S}^2$. Let us suppose that $ s' \neq s$, that  $s' \in ne(s)$, adn that $\mathcal{L}(s) < \mathcal{L}(s')$, the other case is symmetric. \\[0.15cm]
$
\begin{aligned}
    \pi(s)K(s, s') &=  \frac{\mathcal{L}(s)}{\sum_{s \in \mathcal{S}} \mathcal{L}(s)} \times \frac{1}{36} \quad \text{as} \ \mathcal{L}(s) < \mathcal{L}(s') \\
    &=  \frac{\mathcal{L}(s)}{\sum_{s \in \mathcal{S}} \mathcal{L}(s)} \times \frac{1}{36} \times \frac{\mathcal{L}(s')}{\mathcal{L}(s')} \\
    &=  \frac{\mathcal{L}(s')}{\sum_{s \in \mathcal{S}} \mathcal{L}(s)} \times \frac{1}{36} \times \frac{\mathcal{L}(s)}{\mathcal{L}(s')} \\
    &= \pi(s')K(s', s)\\[0.3cm]
\end{aligned}
$

Thus, $\pi$ is \textbf{reversible} for $K$.\\
Let $(s, s') \in \mathcal{S}^2$. Let us note $(a, b, c, d)$ and $(a', b', c', d')$ the elements of $s$ and $s'$.
We note : \\
$s_1 = [a', b, c, d]$\\
$ s_2 = [a', b', c, d]$ \\
$s_3 = [a', b', c', d]$ \\


$
\begin{aligned}
    \mathbb{P}(S_{n+4}=s'\vert S_n = s ) & \geqslant \mathbb{P}(S_{n+4} = s' \cap  S_{n+3}= s_3 \cap S_{n+2} = s_2 \cap S_{n+1} = s_1 \vert S_n = s) \\
    & \geqslant \mathbb{P}(S_{n+4} = s' \vert S_{n+3} = s_3 \cap S_{n+2} = s_2 \cap S_{n+1} = s_1 \cap S_n = s) \\
    & \quad\quad\quad\quad \times \mathbb{P}( S_{n+3} = s_3 \cap S_{n+2} = s_2 \cap S_{n+1} = s_1 \vert S_n = s) \text{  (Baye' s Formula)}\\
    & \geqslant \mathbb{P}(S_{n+4} = s' \vert S_{n+3} = s_3) \times \mathbb{P}( S_{n+3} = s_3 \vert S_{n+2} = s_2 \cap S_{n+1} = s_1 \cap S_n = s) \\
    & \quad\quad\quad\quad \times \mathbb{P}( S_{n+2} = s_2 \cap S_{n+1} = s_1 \vert S_n = s ) \text{ (by Markov's property)}\\
    & \vdots \\
    & \geqslant \mathbb{P}(S_{n+4} = s' \vert S_{n+3} = s_3) \times \mathbb{P}( S_{n+3} = s_3 \vert S_{n+2} = s_2) \times \mathbb{P}( S_{n+2} = s_2 \vert S_{n+1} = s_1) \\
    & \quad\quad\quad\quad \times \mathbb{P}( S_{n+1} = s_1 \vert S_n = s ) \text{ (by Markov's property)}\\
    & \geqslant (\frac{1}{36})^4 \times min (1, \frac{\mathcal{L}(s')}{\mathcal{L}(s)}) \times min (1, \frac{\mathcal{L}(s_3)}{\mathcal{L}(s_2)}) \times min (1, \frac{\mathcal{L}(s_2)}{\mathcal{L}(s_1)}) \times min (1, \frac{\mathcal{L}(s_1)}{\mathcal{L}(s)})  \\
    & > 0 \\
\end{aligned}
$

Thus, $S_n$ is \textbf{irreducible}.\\

A Markov chain of transition matrix $P$ on the support $\mathcal{S}$ is said to be aperiodic if: \\
$\forall s \in \mathcal{S}, \forall s' \in \mathcal{S}, \exists N \in \mathbb{N}, \text{ s.t } \forall n > N, P(s, s')^n >0 $ \\

First, note that $\forall s \in \mathcal{S}$, if $s$ is a local minimum (i.e if $\forall s' \in ne(s), \mathcal{L}(s') > \mathcal{L}(s) $), then we have $K(s,s)=0$\\

By contrapositive, if $s$ is not a local minimum, then $K(s, s)>0$. Moreover, $ \forall s \in \mathcal{S}, \forall s' \in ne(s)$, if $s\neq s'$, then $K(s, s') \neq 0 $\\[0.3cm]
Let $(s, s')  \in \mathcal{S}^2$. \\

\begin{itemize}
    \item If $s'$ is not a local minimum, \\
     $\forall n >4, \mathbb{P}(S_{n} = s' \vert S_0 = s) \geqslant \mathbb{P}(S_{4} = s' \vert S_0 = s) \times K(s', s')^{n-4} > 0 $ \\
    \item If $s'$ is a local minimum,  $\forall s^* \in ne(s')$, $s^*$ is not a local minimum and $K(s^*, s^*) \neq 0$. \\
    $\forall n>5$, $\mathbb{P}(S_n = s' \vert S_0 = s )   \geqslant \mathbb{P}(S_3=s^* \vert S_0 = s ) \times K(s^*, s^*)^{n-4} \times K(s^*, s') >0 $\\
    
\end{itemize}

Thus $S_n$ is an \textbf{aperiodic} Markov Chain.\\


Finally, according to the \textbf{Theorem 5.5 } from \cite*{bodineau2015modelisation}, as $S_n$ is irreductible and aperiodic, as $\pi$ is the stationary distribution, and as $\mathcal{S}$ is countable, $S_n$ converges in distribution to $\pi$.


The most probable set that will be sampled by $S_n$ is so the one that maximises the diversity.
We implemented this MCMC algorithm to maximise $\mathcal{L}_2$ on $\mathcal{S}$ . 
After 200 iterations, the set of parameters that maximised the diversity was \texttt{[0, 5, 10, 12]}. 
A new maxima was found at \texttt{[2, 4, 9, 10] }, which correspond to the parameters \texttt{[sym2sev, asym2rec, rel\_symp\_prob, rel\_severe\_prob]}. 
The $\mathcal{L}_2$- diversity increased from 62353 to 93553. \\

To create the most diverse set possible, we also created 4 different mobilities reports \ref*{fig:mobilities}, corresponding to constant mobility, annual variations, lockdown scenario and the reports from Vasträffik from \cite{gerlee2021predicting}. 
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mobilities.png}
    \caption{Mobilities reports.}
    \label{fig:mobilities}
\end{figure}
ON EN EST LAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

\subsection{Computing confidence intervals on the prediction}

\textbf{Assumption}:
\\[0.5cm]
We suppose that the data of the pandemic observed follows the model $h$, of parameter $\theta^* \in \mathbb{R}^d$. Let $Y_i$, $ i = 1, \ldots, n$ be the number of hospitalized at each day. We suppose that: $Y_i = h_{\theta ^* } (i) + \epsilon_i$, with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$, iid, and independent from all the other variables. The objective is to estimate $\theta^*$. We use $\hat{\theta}$, the least square estimator of $\theta^*$ as an estimator of $\theta^* $:
\[
\hat{\theta} =  \underset{\theta \in \mathbb{R}^d}{\operatorname{argmin}} \sum_{i=1}^{n} (Y_i - h_{\theta}(i))^2
\]

Let:
\begin{align*}
Y &= \begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix} \\
h_\theta &= \begin{pmatrix}
h_\theta(1) \\
\vdots \\
h_\theta(n)
\end{pmatrix}
\end{align*}

We have:
\[
\hat{\theta} =  \underset{\theta \in \mathbb{R}^d}{\operatorname{argmin}}  \left\lVert Y - h_\theta \right\rVert ^2
\]

Now, if $\theta$ is close enough to $\theta^*$, we can write (from \cite*{ruckstuhl2010introduction}):
\[
\forall i \in \{ 1, ..., n\} :  h_\theta(i) = h_{\theta^*} (i ) + (\theta - \theta^*)^T\nabla_\theta h_{\theta^*}(i) 
\]
which leads to:
\[
\hat{\theta} =  \underset{\theta \in \mathbb{R}^d}{\operatorname{argmin}}  \left\lVert Y - h_{\theta^*}  - (\theta - \theta^*)^T\nabla_\theta h_{\theta^*}\right\rVert ^2
\]

Let us define:
\begin{align*}
\tilde{Y} &= Y - h_{\theta^*} \\
\beta &= \theta - \theta^* \\
\hat{\beta} &= \theta - \hat{\theta}
\end{align*}

and let us define the matrix $A \in \mathbb{R} ^{n \times d }$ such that $\forall i \in \{ 1, ..., n\}, \forall j \in \{1, ..., d\}, A_{i, j} = \frac{dh_{\theta^*}}{d\theta_j}(i)$.

The previous problem can be re-written as:
\[
\hat{\beta} =  \underset{\beta \in \mathbb{R}^d}{\operatorname{argmin}}  \left\lVert \tilde{Y} - A \beta \right\rVert ^2
\]

This is a regression linear problem.

Let us solve this problem in the general case.

Let $(A_i, \tilde{Y_i})$ be the observations Let $\mathbb{P}$ be the law from which the $A_i$ are drawn, and let us assume that $Y_i = A_i \beta ^* + \epsilon_i $, with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$.


The solution of this problem is explicitly (from \cite*{powellasymptoticsforleastsquares}):
\[
\hat{\beta } = (A^T A ) ^{-1} A^T \tilde{Y}
\]

This least-square estimator is unbiased:
\[
\mathbb{E}[\hat{\beta}] = \beta^*
\]

\[
\hat {\beta } = \left(  \sum_{i=1}^{n}   A_i ^T A_i \right) ^{-1}    \times \left(  \sum_{i=1}^{n} A_i ^T \tilde{Y_i } \right)
\]

\[
\hat {\beta } =\frac{n}{n} \left(  \sum_{i=1}^{n}   A_i ^T A_i \right) ^{-1}    \times \left(  \sum_{i=1}^{n} A_i ^T \tilde{Y_i } \right)
\]

\[
\hat {\beta } = \left( \frac{1}{n} \sum_{i=1}^{n}   A_i ^T A_i \right) ^{-1}    \times \left( \frac{1}{n} \sum_{i=1}^{n} A_i ^T \tilde{Y_i } \right)
\]

Let us denote:
\[
\hat{D}  = \frac{1}{n} \sum_{i=1}^{n}   A_i ^T A_i , \quad \text{and} \quad \hat{\delta}  = \left( \frac{1}{n} \sum_{i=1}^{n} A_i ^T \tilde{Y_i } \right)
\]

We have:
\[
\hat{\beta} = \hat{D} ^{-1} \hat{\delta}
\]

\[
\hat{D}  \underset{a.s}{\rightarrow} D=  \mathbb{E}[A_i ^T A_i ]
\]

\[
\hat{\delta}   \underset{a.s}{\rightarrow}  \delta = \mathbb{E}[A_i ^T \tilde{Y}_i ]
\]

$ \hat{\beta} = \hat{D}^{-1} \hat{\delta}  \underset{a.s}{\rightarrow}  D^{-1} \delta $ , as the following function $\phi$ is continuous:
\[
\phi: \left\{
\begin{array}{rcl}
\mathcal{GL}_n(\mathbb{R}) & \to & \mathcal{GL}_n(\mathbb{R}) \\
A & \mapsto & A^{-1}
\end{array}
\right.
\]


Now, let us show that $\hat{\beta}$ is asymptotically normal: 

\begin{align*}
    \sqrt{n} ( \hat{\beta} - \beta ^* ) &= \sqrt{n} (\hat{D}^{-1} \hat{\delta} - \beta ^* ) \\
     &= \sqrt{n} (\hat{D}^{-1} \hat{\delta} - \hat{D}^{-1} \hat{D} \beta ^* ) \\
     &= \sqrt{n} \hat{D}^{-1} (\hat{\delta} -  \hat{D} \beta ^* )  \\
     &= \sqrt{n} \hat{D}^{-1} \left( \frac{1}{n} \sum_{i=1}^{n} A_i ^T \tilde{Y_i} - \frac{1}{n}  \sum_{i=1}^{n}  A_i ^T A_i \beta ^* \right)  \\
     &= \frac{\sqrt{n}}{n} \hat{D}^{-1} \left( \sum_{i=1}^{n} A_i ^T (\tilde{Y_i} -    A_i \beta ^* ) \right)   \\
     &= \frac{1}{\sqrt{n}}  \hat{D} ^{-1} \left( \sum_{i=1}^{n}  A_i ^T \epsilon _ i \right)  
\end{align*}

This line is made of two terms. Let's show that each one of them converges in law. 

\begin{align*}
    \frac{1}{\sqrt{n}} \left( \sum_{i=1}^{n}  A_i^T \epsilon'_i \right) &= \sqrt{n} \left( \frac{1}{n} \sum_{i=1}^{n}  A_i^T \epsilon'_i \right) \\
    &= \sqrt{n} \left( \frac{1}{n} \sum_{i=1}^{n}  A_i^T \epsilon'_i - 0 \right) \\
    &\xrightarrow{\mathcal{L}} \mathcal{N}(0, \text{Var}(A_i^T \epsilon_i) )
\end{align*}

Yet, as $\epsilon_i $ and $  A_i $ are independant, and $\mathbb{E} [ A_i^T \epsilon'_i] = 0 $ , $\text{Var}(A_i^T \epsilon_i) = \mathbb{E}[A_i A_i ^T \epsilon_i ^2 ] = \mathbb{E} [ A_i A_i ^T ] \sigma'^2$. 

Finally, $\frac{1}{\sqrt{n}} \left( \sum_{i=1}^{n}  A_i^T \epsilon'_i \right) \xrightarrow{\mathcal{L}} \mathcal{N}(0, D \sigma'^{2})$. 

On the other hand, $\hat{D}^{-1} \xrightarrow{\mathcal{L}} D^{-1} $, which is constant.

Finally, with Slutsky, we obtain that: 

\begin{align*}
    \sqrt{n} (\hat{\beta} - \beta^*) &\xrightarrow{\mathcal{L}} D^{-1}\mathcal{N}(0, D \sigma'^2) \\
    &\xrightarrow{\mathcal{L}} \mathcal{N}(0, D^{-1} (D \sigma'^2) (D^{-1})^T) \\
    &\xrightarrow{\mathcal{L}} \mathcal{N}(0, D^{-1} D \sigma'^2 (D^{-1})^{T}) \\
    &\xrightarrow{\mathcal{L}} \mathcal{N}(0, \sigma'^2 D^{-1}) \\
    &\xrightarrow{\mathcal{L}} \mathcal{N}(0, \sigma'^2 (A^T A)^{-1})
\end{align*}


Let's get back to the first problem: 

As $\beta ^* = 0 $ and $\hat{\beta} = \hat{\theta} - \theta ^* $, we have: 

\[
\sqrt{n} (\hat{\theta} - \theta^*) \xrightarrow{\mathcal{L}} \mathcal{N}(0, \sigma^2 (A^T A)^{-1})
\]

and, 
\[
\hat{\theta} \sim \mathcal{N}(\theta^*, \frac{\sigma^2}{n} (A^T A)^{-1})
\]

As a first conclusion, we have that $\hat{\theta}$ is asymptotically normal.

Let $\Sigma$ be the covariance matrix estimated from the computation of $\hat{\theta}$. In our case, $\Sigma = \frac{\sigma^2}{n} (A^T A)^{-1}$. 

As $\hat{\theta}$ is asymptotically normal, we can apply the delta-method: 

\[
\sqrt{n} (\hat{\theta} -\theta^*) \xrightarrow{\mathcal{L}} \mathcal{N}(0, \ \Sigma)
\]
\[
\sqrt{n} (h_{\hat{\theta}} -h_{\theta^*}) \xrightarrow{\mathcal{L}} \mathcal{N}(0, \nabla_\theta h_\theta ^T \Sigma  \nabla_\theta h_\theta)
\]

And finally: 
\[
h_{\hat{\theta}} \rightarrow \mathcal{N}(h_{\theta^*}, \frac{1}{n}\nabla_\theta h_\theta ^T \Sigma  \nabla_\theta h_\theta)
\]

By estimating $\frac{1}{n} \Sigma$ from \texttt{curve\_fit}, we can compute the confidence interval of the prediction with the quantiles of the normal distribution.
